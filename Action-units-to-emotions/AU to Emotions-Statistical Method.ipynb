{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Introduction\n",
    "\n",
    "* AUs are mapped to the six basic emotions using a learned statistical relationship and a suitable matching technique.\n",
    "\n",
    "* Relationships between the AUs and emotions are captured as template strings comprising the most discriminative AUs for each emotion.\n",
    "\n",
    "* The template strings are computed using a concept called discriminative power.\n",
    "\n",
    "* The Longest Common Subsequence (LCS) distance, an approach for approximate string matching, is applied to calculate the closeness of a test string of AUs with the template strings, and hence infer the underlying emotions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Emotion recognition from facial cues based on FACS rules can be classified as: \n",
    "     1. single-phase, where emotions are recognized directly and \n",
    "     2. two-phase, where the facial action units (AUs)\n",
    "     \n",
    "which are considered as building blocks of facial expressions, are detected first, then the output emotion is inferred from the detected AUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Valstar et al have formulated mapping rules based on EMFACS (Emotional FACS). They also presented an Artificial Neural Network (ANN) based method which is similar to Piat et al’s work for examining the underlying emotions from facial expressions. Chang et al used partially-observed hidden conditional random fields for facial expression recognition. Based on the various combinations of 15 most frequently occurring AUs, they prepared an extensive set of 100 hidden labels within their graphical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Considering such practical issues, we present a method that approaches AU to emotion mapping as a problem of approximate string matching while using learned statistics that quantify the match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.The Proposed Method\n",
    "\n",
    "* The aligned faces are rescaled to the size of 96x96 pixels. Gabor filter bank of size 56 (7 scales, 8 orientations) is applied on each cropped face template to extract the feature vectors of size 96x96x56. AdaBoost as a feature selection method is applied to reduce the feature vector dimension. We use Support Vector Machines (SVM) to model the AUs.\n",
    "\n",
    "![](systemflowchart.png)\n",
    "\n",
    "* Based on our detailed study of FACS, we chose 15 AUs (See Table 1) that are found to be sufficient in representing the six basic emotions (Anger, Fear, Happy, Sad, Surprise, Disgust). 15 SVM classifiers are trained to model each of the 15 selected AUs.\n",
    "\n",
    "![](table1.png)\n",
    "\n",
    "* Each SVM classifier outputs the presence or absence of its corresponding AU according to the intensity of the AUs present in the face (test image). The detected facial AUs are then processed by the AU to emotion mapping module to predict the final emotion state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Inferring Emotions From Action Units\n",
    "To generalize the solution to robust scenario, we present a principled approach to infer emotions from AUs. The method consist of two parts:\n",
    "1. Deriving AU-Emotion relationship\n",
    "2. Predicting the emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The relations are obtained in the form of a relation matrix which is derived using a concept called discriminative power . The discriminative power is defined as,\n",
    " \n",
    " ![](eq1.png)\n",
    " \n",
    "*  The magnitude of H quantifies the discriminative power of an AU for an emotion, and the sign depicts whether an AU increases or decreases the probability of mapping to an emotion. The relation matrix is derived by normalizing H across all the AUs for each of the emotions.\n",
    "\n",
    "* For each emotion, we select the top N entries of highly discriminative AUs and store them as template strings of AUs for future matching. We use template string length (N) of 5 for our experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given a test string of AUs, it is matched against the template strings to find the emotion. We use Longest Common Subsequence (LCS) to measure the similarity between the strings.\n",
    "\n",
    "* LCS allows only ‘insertion‘ and ‘deletion‘, but not (or conditional) ‘replacement‘ of units between the strings. This characteristic of LCS is found to be very suitable for our problem of predicting emotions from strings of AUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Experiments and Results\n",
    "#### 4.1 Databases:\n",
    "* We use a portion (580 images) of CK+ database labeled for AU and emotion to learn the statistical relationship between the AUs and emotions.\n",
    "\n",
    "#### 4.2 Results:\n",
    "* For comparison, we implemented EMFACS based technique present in and Moon et al’s method which are based on deterministic rules for mapping AUs to emotions. The correct detection rate with respect to the ground truth emotion labels is used as a metric for performance measurement.\n",
    "\n",
    "\n",
    "* The primary advantage of the proposed method as compared to the rigid rule based techniques is that, the method does not require a large look-up tables to accommodate combinations of AUs that may occur due to errors in AU detection. Also, the proposed method of non-rigid template AUs with weight association helps to infer non-peak emotions and as well handles errors in AU detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the AUs separated by space:12 10 14 6 7\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-eea876edb976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#module to take the input of the present AUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the AUs separated by space:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The person seems to be\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-eea876edb976>\u001b[0m in \u001b[0;36memotion\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlcs_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlcs_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmax_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlcs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcs_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "def lcs(X, Y, m, n): \n",
    "    if m == 0 or n == 0: \n",
    "       return 0; \n",
    "    elif X[m-1] == Y[n-1]: \n",
    "       return 1 + lcs(X, Y, m-1, n-1); \n",
    "    else: \n",
    "       return max(lcs(X, Y, m, n-1), lcs(X, Y, m-1, n)); \n",
    "\n",
    "#the AU list\n",
    "happy=['12','25','6']\n",
    "sad=['4','15','17','1','6']\n",
    "fear=['1','4','20','25','5']\n",
    "angry=['4','7','24','17','23']\n",
    "suprise=['1','2','25','26','5']\n",
    "disgust=['9','10','17','4','24']\n",
    "\n",
    "emotion_list=[happy.sort(),sad.sort(),fear.sort(),angry.sort(),suprise.sort(),disgust.sort()]\n",
    "\n",
    "print(emotion_list)\n",
    "emotions=['happy','sad','fear','angry','suprise','disgust']\n",
    "\n",
    "def emotion(X):\n",
    "    lcs_list=[]\n",
    "    for y in emotion_list:\n",
    "        lcs_list.append(lcs(X,y,len(X),len(y)))\n",
    "    max_value=max(lcs_list)\n",
    "    indexes=lcs_list.index(max_value)\n",
    "    \n",
    "    return emotions[indexes]\n",
    "\n",
    "#module to take the input of the present AUs\n",
    "input_list=input(\"Enter the AUs separated by space:\").split()\n",
    "print(\"The person seems to be\",emotion(input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#module to take the input of the present AUs\n",
    "input_list=input(\"Enter the AUs separated by space:\").split()\n",
    "print(\"The person seems to be\",emotion(input_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
